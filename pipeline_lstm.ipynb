{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from os import path\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from string import punctuation\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm_notebook\n",
    "from unidecode import unidecode\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"../data/\"\n",
    "LANGUAGE = \"spanish\"\n",
    "DROP_COLUMNS = [\"split\", \"language\"]\n",
    "UNRELIABLE_SAMPLING = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02086b1e4761468da39a1f97ad1c8c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 27s, sys: 19.1 s, total: 1min 46s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_data(base_path, language, drop_columns, unreliable_sampling):\n",
    "    datasets = {}\n",
    "    for ds in tqdm_notebook([\"train_reliable\", \"train_unreliable\", \"dev\", \"test\"]):\n",
    "        if ds == \"train_unreliable\" and unreliable_sampling == 0:\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_parquet(\n",
    "            path.join(base_path, f\"{language}\", f\"{ds}.parquet\")\n",
    "        ).drop(drop_columns, axis=1, errors=\"ignore\")\n",
    "        \n",
    "        if ds == \"train_unreliable\" and 0 < unreliable_sampling < 1:\n",
    "            df = df.groupby([\"category\"]).apply(\n",
    "                lambda cat: cat.sample(frac=unreliable_sampling)\n",
    "            ).reset_index(drop=True)\n",
    "        elif ds == \"train_unreliable\" and unreliable_sampling > 1:\n",
    "            df = df.groupby([\"category\"]).apply(\n",
    "                lambda cat: cat.sample(n=int(unreliable_sampling))\n",
    "            ).reset_index(drop=True)\n",
    "        \n",
    "        if ds == \"train_reliable\":\n",
    "            datasets[\"train\"] = df\n",
    "        elif ds == \"train_unreliable\":\n",
    "            datasets[\"train\"] = pd.concat([\n",
    "                datasets[\"train\"],\n",
    "                df\n",
    "            ], ignore_index=True)\n",
    "        else:\n",
    "            datasets[ds] = df\n",
    "    \n",
    "    w2v = KeyedVectors.load_word2vec_format(\n",
    "        path.join(base_path, f\"{language}\", \"word2vec.bin.gz\"), \n",
    "        binary=True\n",
    "    )\n",
    "    \n",
    "    return datasets, w2v\n",
    "\n",
    "datasets, w2v = load_data(DIR_PATH, LANGUAGE, DROP_COLUMNS, UNRELIABLE_SAMPLING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.1 s, sys: 1.54 s, total: 11.7 s\n",
      "Wall time: 5.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def label_encoder(*dfs):\n",
    "    labels = pd.concat(dfs)[\"category\"].tolist()\n",
    "    lbl_enc = LabelEncoder().fit(labels)\n",
    "\n",
    "    return lbl_enc\n",
    "\n",
    "lbl_enc = label_encoder(datasets[\"train\"], datasets[\"dev\"])\n",
    "\n",
    "for split in [\"train\", \"dev\"]:\n",
    "    datasets[split][\"target\"] = lbl_enc.transform(datasets[split][\"category\"])\n",
    "    datasets[split].drop([\"category\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288bfe9bd9984f61897b8691f5cef250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 13.4 s, sys: 786 ms, total: 14.2 s\n",
      "Wall time: 12.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def remove_punctuation(datasets, punctuation, column=\"tokens\"):\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        datasets[split][\"non_punct_tokens\"] = datasets[split][column].apply(\n",
    "            lambda words: [w for w in words if w not in punctuation]\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "datasets = remove_punctuation(datasets, punctuation, \"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c53e48bcfe6d44aa9d6385396e0fc8d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.67 s, sys: 240 ms, total: 6.91 s\n",
      "Wall time: 6.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def remove_stopwords(datasets, stopwords, column=\"tokens\"):\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        datasets[split][\"non_sw_tokens\"] = datasets[split][column].apply(\n",
    "            lambda words: [w for w in words if w not in stopwords]\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "datasets = remove_stopwords(datasets, set(stopwords.words(LANGUAGE)), \"non_punct_tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6bf000816044f4fae47c65fd9e5e40b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 2min 39s, sys: 1.49 s, total: 2min 41s\n",
      "Wall time: 2min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def word_with_vector(word, w2v, stemmer):\n",
    "    if word in w2v:\n",
    "        return word\n",
    "    elif word.capitalize() in w2v:\n",
    "        return word.capitalize()\n",
    "    elif unidecode(word) in w2v:\n",
    "        return unidecode(word)\n",
    "    elif unidecode(word.capitalize()) in w2v:\n",
    "        return unidecode(word.capitalize())\n",
    "    elif stemmer.stem(word) in w2v:\n",
    "        return stemmer.stem(word)\n",
    "    elif word.isdigit():\n",
    "        return \"DIGITO\"\n",
    "    else:\n",
    "        return \"<UNK>\"\n",
    "    # TODO: Lemmatization? Other normalizations?\n",
    "\n",
    "def word_vectorize(datasets, language, w2v, column=\"tokens\"):\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        datasets[split][\"normalized_title\"] = datasets[split][column].apply(\n",
    "            lambda words: [word_with_vector(w, w2v, stemmer) for w in words]\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "datasets = word_vectorize(datasets, LANGUAGE, w2v, \"non_punct_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 118362\n",
      "CPU times: user 15.8 s, sys: 1.03 s, total: 16.8 s\n",
      "Wall time: 13.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def words_to_idx(all_words, w2v, null_token=\"<NULL>\",\n",
    "                 unknown_token=\"<UNK>\", num_token=\"DIGITO\"):\n",
    "    word_index = {word for words in all_words for word in words if word in w2v}\n",
    "    word_index = {word: idx for idx, word in enumerate(sorted(word_index), start=1)}\n",
    "    word_index[null_token] = 0\n",
    "    if num_token not in word_index:\n",
    "        word_index[num_token] = len(word_index)\n",
    "    word_index[unknown_token] = len(word_index)\n",
    "\n",
    "    return word_index\n",
    "\n",
    "word_index = words_to_idx(pd.concat(list(datasets.values()), sort=False)[\"normalized_title\"], w2v)\n",
    "\n",
    "print(f\"Vocab length: {len(word_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.1 s, sys: 676 ms, total: 24.7 s\n",
      "Wall time: 24.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "MAX_SEQUENCE_LEN = 8\n",
    "\n",
    "def sequence_padding(series, word_index, max_len):\n",
    "    return pad_sequences(\n",
    "            series.apply(\n",
    "                lambda words: [word_index.get(word, word_index[\"<UNK>\"]) for word in words]\n",
    "            ).tolist(), maxlen=max_len\n",
    "        )\n",
    "\n",
    "train_word_sequences = sequence_padding(\n",
    "    datasets[\"train\"][\"normalized_title\"], word_index, MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "dev_word_sequences = sequence_padding(\n",
    "    datasets[\"dev\"][\"normalized_title\"], word_index, MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "test_word_sequences = sequence_padding(\n",
    "    datasets[\"test\"][\"normalized_title\"], word_index, MAX_SEQUENCE_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 442 ms, sys: 3.79 s, total: 4.23 s\n",
      "Wall time: 4.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = to_categorical(\n",
    "    datasets[\"train\"][\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")\n",
    "\n",
    "dev_target = to_categorical(\n",
    "    datasets[\"dev\"][\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 460 ms, sys: 160 ms, total: 620 ms\n",
      "Wall time: 618 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_embedding_matrix(word_index, w2v):\n",
    "    embedding_matrix = np.zeros((len(word_index), w2v.vector_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in w2v and word not in {\"<NULL>\", \"<UNK>\", \"<NUM>\"}:\n",
    "            embedding_matrix[i] = w2v[word]\n",
    "        elif word == \"<UNK>\" or word == \"<NUM>\":\n",
    "            embedding_matrix[i] = np.random.normal(size=(w2v.vector_size,))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "word_embedding_matrix = get_embedding_matrix(word_index, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_UNITS = [128, 128]\n",
    "# DROPOUT = [0.1, 0.1]\n",
    "BIDIRECTIONAL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "embedding_4 (Embedding)      (None, 8, 300)            35508600  \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 8, 256)            439296    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1573)              404261    \n",
      "=================================================================\n",
      "Total params: 36,746,397\n",
      "Trainable params: 1,237,797\n",
      "Non-trainable params: 35,508,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(word_vocab_size, word_vector_size, word_embedding_matrix, output_size, max_sequence_len,\n",
    "                lstm_units, bidirectional):\n",
    "    word_embedding_layer = Embedding(word_vocab_size, word_vector_size,\n",
    "                                     weights=[word_embedding_matrix],\n",
    "                                     input_length=max_sequence_len,\n",
    "                                     trainable=False)\n",
    "\n",
    "    word_sequence_input = Input(shape=(max_sequence_len,))\n",
    "    layer = word_embedding_layer(word_sequence_input)\n",
    "\n",
    "    for idx, units in enumerate(lstm_units[:-1]):\n",
    "        if bidirectional:\n",
    "            layer = Bidirectional(LSTM(units, return_sequences=len(lstm_units) > 1))(layer)\n",
    "        else:\n",
    "            layer = LSTM((units))(layer)\n",
    "    \n",
    "    if bidirectional:\n",
    "        layer = Bidirectional(LSTM(lstm_units[-1]))(layer)\n",
    "    else:\n",
    "        layer = LSTM(lstm_units[-1])(layer)\n",
    "    \n",
    "    preds = Dense(output_size, activation=\"softmax\")(layer)\n",
    "    model = Model(word_sequence_input, preds)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    word_vocab_size=len(word_index),\n",
    "    word_vector_size=w2v.vector_size, \n",
    "    word_embedding_matrix=word_embedding_matrix,\n",
    "    output_size=lbl_enc.classes_.shape[0],\n",
    "    max_sequence_len=MAX_SEQUENCE_LEN,\n",
    "    lstm_units=LSTM_UNITS,\n",
    "    bidirectional=BIDIRECTIONAL\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2725070 samples, validate on 499625 samples\n",
      "Epoch 1/15\n",
      "2725070/2725070 [==============================] - 137s 50us/sample - loss: 1.2885 - accuracy: 0.7290 - val_loss: 1.4143 - val_accuracy: 0.7077\n",
      "Epoch 2/15\n",
      "2725070/2725070 [==============================] - 147s 54us/sample - loss: 1.2370 - accuracy: 0.7378 - val_loss: 1.3849 - val_accuracy: 0.7133\n",
      "Epoch 3/15\n",
      "2725070/2725070 [==============================] - 143s 52us/sample - loss: 1.1952 - accuracy: 0.7452 - val_loss: 1.3622 - val_accuracy: 0.7171\n",
      "Epoch 4/15\n",
      "2725070/2725070 [==============================] - 139s 51us/sample - loss: 1.1596 - accuracy: 0.7515 - val_loss: 1.3452 - val_accuracy: 0.7208\n",
      "Epoch 5/15\n",
      "2725070/2725070 [==============================] - 130s 48us/sample - loss: 1.1287 - accuracy: 0.7571 - val_loss: 1.3293 - val_accuracy: 0.7244\n",
      "Epoch 6/15\n",
      "2725070/2725070 [==============================] - 129s 47us/sample - loss: 1.1018 - accuracy: 0.7618 - val_loss: 1.3157 - val_accuracy: 0.7265\n",
      "Epoch 7/15\n",
      "2725070/2725070 [==============================] - 131s 48us/sample - loss: 1.0774 - accuracy: 0.7661 - val_loss: 1.3060 - val_accuracy: 0.7286\n",
      "Epoch 8/15\n",
      "2725070/2725070 [==============================] - 126s 46us/sample - loss: 1.0556 - accuracy: 0.7701 - val_loss: 1.2886 - val_accuracy: 0.7319\n",
      "Epoch 9/15\n",
      "2725070/2725070 [==============================] - 145s 53us/sample - loss: 1.0350 - accuracy: 0.7737 - val_loss: 1.2874 - val_accuracy: 0.7321\n",
      "Epoch 10/15\n",
      "1384448/2725070 [==============>...............] - ETA: 1:03 - loss: 1.0072 - accuracy: 0.7787"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=train_word_sequences,\n",
    "    y=train_target,\n",
    "    batch_size=4096,\n",
    "    epochs=15,\n",
    "    validation_data=(dev_word_sequences, dev_target),\n",
    "    validation_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"dev\"][\"predictions\"] = model.predict(\n",
    "    dev_word_sequences, batch_size=1024, verbose=0\n",
    ").argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(datasets[\"dev\"][\"target\"], datasets[\"dev\"][\"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
