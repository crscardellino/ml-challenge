{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from os import path\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from string import punctuation\n",
    "from tensorflow.keras.layers import (BatchNormalization, Concatenate, Conv1D, Dense, Dropout, Embedding, \n",
    "                                     GlobalMaxPooling1D, Input, TimeDistributed)\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm_notebook\n",
    "from unidecode import unidecode\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"../data/\"\n",
    "LANGUAGE = \"spanish\"\n",
    "DROP_COLUMNS = [\"split\", \"language\"]\n",
    "UNRELIABLE_SAMPLING = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244faf11271147b2a40096ce6a2c2801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 19s, sys: 20.6 s, total: 1min 40s\n",
      "Wall time: 1min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_data(base_path, language, drop_columns, unreliable_sampling):\n",
    "    datasets = {}\n",
    "    for ds in tqdm_notebook([\"train_reliable\", \"train_unreliable\", \"dev\", \"test\"]):\n",
    "        if ds == \"train_unreliable\" and unreliable_sampling == 0:\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_parquet(\n",
    "            path.join(base_path, f\"{language}\", f\"{ds}.parquet\")\n",
    "        ).drop(drop_columns, axis=1, errors=\"ignore\")\n",
    "        \n",
    "        if ds == \"train_unreliable\" and 0 < unreliable_sampling < 1:\n",
    "            df = df.groupby([\"category\"]).apply(\n",
    "                lambda cat: cat.sample(frac=unreliable_sampling)\n",
    "            ).reset_index(drop=True)\n",
    "        elif ds == \"train_unreliable\" and unreliable_sampling > 1:\n",
    "            df = df.groupby([\"category\"]).apply(\n",
    "                lambda cat: cat.sample(n=int(unreliable_sampling))\n",
    "            ).reset_index(drop=True)\n",
    "        \n",
    "        if ds == \"train_reliable\":\n",
    "            datasets[\"train\"] = df\n",
    "        elif ds == \"train_unreliable\":\n",
    "            datasets[\"train\"] = pd.concat([\n",
    "                datasets[\"train\"],\n",
    "                df\n",
    "            ], ignore_index=True)\n",
    "        else:\n",
    "            datasets[ds] = df\n",
    "    \n",
    "    w2v = KeyedVectors.load_word2vec_format(\n",
    "        path.join(base_path, f\"{language}\", \"word2vec.bin.gz\"), \n",
    "        binary=True\n",
    "    )\n",
    "    \n",
    "    return datasets, w2v\n",
    "\n",
    "datasets, w2v = load_data(DIR_PATH, LANGUAGE, DROP_COLUMNS, UNRELIABLE_SAMPLING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.86 s, sys: 1.27 s, total: 10.1 s\n",
      "Wall time: 2.97 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def label_encoder(*dfs):\n",
    "    labels = pd.concat(dfs)[\"category\"].tolist()\n",
    "    lbl_enc = LabelEncoder().fit(labels)\n",
    "\n",
    "    return lbl_enc\n",
    "\n",
    "lbl_enc = label_encoder(datasets[\"train\"], datasets[\"dev\"])\n",
    "\n",
    "for split in [\"train\", \"dev\"]:\n",
    "    datasets[split][\"target\"] = lbl_enc.transform(datasets[split][\"category\"])\n",
    "    datasets[split].drop([\"category\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38ae7dd4b17405fb8e75ec17324b0cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.2 s, sys: 709 ms, total: 18.9 s\n",
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def remove_punctuation(datasets, punctuation, tcolumn=\"tokens\", pcolumn=\"pos\"):\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        token_pos = pd.Series(list(zip(datasets[split][tcolumn], datasets[split][pcolumn])))\n",
    "        datasets[split][[\"non_punct_tokens\", \"non_punct_pos\"]] = pd.DataFrame(\n",
    "            token_pos.apply(\n",
    "                lambda tw: list(zip(*[(tw[0][i], tw[1][i]) for i in range(len(tw[0])) if tw[0][i] not in punctuation]))\n",
    "            ).tolist()\n",
    "        )\n",
    "        datasets[split] = datasets[split].dropna()\n",
    "    return datasets\n",
    "\n",
    "datasets = remove_punctuation(datasets, punctuation, \"words\", \"pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb06d63462a24ed28f6ca24b0e78ed8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.3 s, sys: 1.14 s, total: 21.5 s\n",
      "Wall time: 13.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def remove_stopwords(datasets, stopwords, tcolumn=\"tokens\", pcolumn=\"pos\"):\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        token_pos = pd.Series(list(zip(datasets[split][tcolumn], datasets[split][pcolumn])))\n",
    "        datasets[split][[\"non_sw_tokens\", \"non_sw_pos\"]] = pd.DataFrame(\n",
    "            token_pos.apply(\n",
    "                lambda tw: list(zip(*[(tw[0][i], tw[1][i]) \n",
    "                                      for i in range(len(tw[0])) if tw[0][i]]))\n",
    "            ).tolist()\n",
    "        )\n",
    "        datasets[split] = datasets[split].dropna()\n",
    "    return datasets\n",
    "\n",
    "datasets = remove_stopwords(datasets, set(stopwords.words(LANGUAGE)), \"non_punct_tokens\", \"non_punct_pos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c8b3ac4c5a4c21a8b912dd12187dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 674 ms, total: 1min 26s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def word_with_vector(word, w2v, stemmer):\n",
    "    if word in w2v:\n",
    "        return word\n",
    "    elif word.capitalize() in w2v:\n",
    "        return word.capitalize()\n",
    "    elif unidecode(word) in w2v:\n",
    "        return unidecode(word)\n",
    "    elif unidecode(word.capitalize()) in w2v:\n",
    "        return unidecode(word.capitalize())\n",
    "    elif stemmer.stem(word) in w2v:\n",
    "        return stemmer.stem(word)\n",
    "    elif word.isdigit():\n",
    "        return \"DIGITO\"\n",
    "    else:\n",
    "        return \"<UNK>\"\n",
    "    # TODO: Lemmatization? Other normalizations?\n",
    "\n",
    "def word_vectorize(datasets, language, w2v, column=\"tokens\"):\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        datasets[split][\"normalized_tokens\"] = datasets[split][column].apply(\n",
    "            lambda words: [word_with_vector(w, w2v, stemmer) for w in words]\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "datasets = word_vectorize(datasets, LANGUAGE, w2v, \"non_sw_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 100469\n",
      "CPU times: user 11 s, sys: 598 ms, total: 11.6 s\n",
      "Wall time: 7.17 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def words_to_idx(all_words, w2v, null_token=\"<NULL>\",\n",
    "                 unknown_token=\"<UNK>\", num_token=\"DIGITO\"):\n",
    "    word_index = {word for words in all_words for word in words if word in w2v}\n",
    "    word_index = {word: idx for idx, word in enumerate(sorted(word_index), start=1)}\n",
    "    word_index[null_token] = 0\n",
    "    if num_token not in word_index:\n",
    "        word_index[num_token] = len(word_index)\n",
    "    word_index[unknown_token] = len(word_index)\n",
    "\n",
    "    return word_index\n",
    "\n",
    "word_index = words_to_idx(pd.concat(list(datasets.values()), sort=False)[\"normalized_tokens\"], w2v)\n",
    "\n",
    "print(f\"Vocab length: {len(word_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Char vocab length: 135\n",
      "CPU times: user 6.6 s, sys: 188 ms, total: 6.79 s\n",
      "Wall time: 4.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def chars_to_idx(titles, null_token=\"<NULL>\", unknown_token=\"<UNK>\"):\n",
    "    char_index = {char for title in titles for char in title}\n",
    "    char_index = {char: idx for idx, char in enumerate(sorted(char_index), start=1)}\n",
    "    char_index[null_token] = 0\n",
    "    char_index[unknown_token] = len(char_index)\n",
    "\n",
    "    return char_index\n",
    "\n",
    "char_index = chars_to_idx(\n",
    "    pd.concat(\n",
    "        list(datasets.values()), \n",
    "        ignore_index=True, \n",
    "        sort=False\n",
    "    )[\"non_sw_tokens\"].apply(lambda tokens: \" \".join(tokens))\n",
    ")\n",
    "\n",
    "print(f\"Char vocab length: {len(char_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos TAG length: 19\n",
      "CPU times: user 6.21 s, sys: 373 ms, total: 6.58 s\n",
      "Wall time: 2.15 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def pos_tags_to_idx(all_pos_tags, w2v, null_pos_tag=\"<NULL>\", unknown_pos_tag=\"<UNK>\"):\n",
    "    pos_tag_index = {pos_tag for pos_tags in all_pos_tags for pos_tag in pos_tags}\n",
    "    pos_tag_index = {pos_tag: idx for idx, pos_tag in enumerate(sorted(pos_tag_index), start=1)}\n",
    "    pos_tag_index[null_pos_tag] = 0\n",
    "    pos_tag_index[unknown_pos_tag] = len(pos_tag_index)\n",
    "\n",
    "    return pos_tag_index\n",
    "\n",
    "pos_tag_index = pos_tags_to_idx(pd.concat(list(datasets.values()), sort=False)[\"non_sw_pos\"], w2v)\n",
    "\n",
    "print(f\"Pos TAG length: {len(pos_tag_index)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word and PoS Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.8 s, sys: 228 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "WORD_MAX_SEQUENCE_LEN = 15\n",
    "\n",
    "def word_sequence_padding(series, word_index, max_len):\n",
    "    return pad_sequences(\n",
    "            series.apply(\n",
    "                lambda words: [word_index.get(word, word_index[\"<UNK>\"]) for word in words]\n",
    "            ).tolist(), maxlen=max_len\n",
    "        )\n",
    "\n",
    "train_word_sequences = word_sequence_padding(\n",
    "    datasets[\"train\"][\"normalized_tokens\"], word_index, WORD_MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "dev_word_sequences = word_sequence_padding(\n",
    "    datasets[\"dev\"][\"normalized_tokens\"], word_index, WORD_MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "test_word_sequences = word_sequence_padding(\n",
    "    datasets[\"test\"][\"normalized_tokens\"], word_index, WORD_MAX_SEQUENCE_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 328 ms, sys: 88 ms, total: 416 ms\n",
      "Wall time: 437 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_embedding_matrix(word_index, w2v):\n",
    "    embedding_matrix = np.zeros((len(word_index), w2v.vector_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in w2v and word not in {\"<NULL>\", \"<UNK>\", \"<NUM>\"}:\n",
    "            embedding_matrix[i] = w2v[word]\n",
    "        elif word == \"<UNK>\" or word == \"<NUM>\":\n",
    "            embedding_matrix[i] = np.random.normal(size=(w2v.vector_size,))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "word_embedding_matrix = get_embedding_matrix(word_index, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.5 s, sys: 356 ms, total: 11.9 s\n",
      "Wall time: 11.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def pos_tag_sequence_padding(series, pos_tag_index, max_len):\n",
    "    return pad_sequences(\n",
    "            series.apply(\n",
    "                lambda pos_tags: [pos_tag_index.get(pos_tag, pos_tag_index[\"<UNK>\"]) for pos_tag in pos_tags]\n",
    "            ).tolist(), maxlen=max_len\n",
    "        )\n",
    "\n",
    "train_pos_tag_sequences = pos_tag_sequence_padding(\n",
    "    datasets[\"train\"][\"non_sw_pos\"], pos_tag_index, WORD_MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "dev_pos_tag_sequences = pos_tag_sequence_padding(\n",
    "    datasets[\"dev\"][\"non_sw_pos\"], pos_tag_index, WORD_MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "test_pos_tag_sequences = pos_tag_sequence_padding(\n",
    "    datasets[\"test\"][\"non_sw_pos\"], pos_tag_index, WORD_MAX_SEQUENCE_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 31s, sys: 762 ms, total: 1min 32s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "CHAR_MAX_SEQUENCE_LEN = 10\n",
    "\n",
    "def char_sequence_padding(series, char_index, char_max_len, word_max_len):\n",
    "    return pad_sequences(\n",
    "        series.apply(\n",
    "            lambda words: pad_sequences([\n",
    "                [char_index.get(char, char_index[\"<UNK>\"]) for char in word]\n",
    "            for word in words], maxlen=char_max_len)\n",
    "    ), maxlen=word_max_len, value=np.zeros(char_max_len))\n",
    "\n",
    "train_char_sequences = char_sequence_padding(\n",
    "    datasets[\"train\"][\"non_sw_tokens\"], char_index, CHAR_MAX_SEQUENCE_LEN, WORD_MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "dev_char_sequences = char_sequence_padding(\n",
    "    datasets[\"dev\"][\"non_sw_tokens\"], char_index, CHAR_MAX_SEQUENCE_LEN, WORD_MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "test_char_sequences = char_sequence_padding(\n",
    "    datasets[\"test\"][\"non_sw_tokens\"], char_index, CHAR_MAX_SEQUENCE_LEN, WORD_MAX_SEQUENCE_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 273 ms, sys: 2.15 s, total: 2.42 s\n",
      "Wall time: 2.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = to_categorical(\n",
    "    datasets[\"train\"][\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")\n",
    "\n",
    "dev_target = to_categorical(\n",
    "    datasets[\"dev\"][\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_FILTERS_LEN = [2, 3, 4, 5]\n",
    "WORD_FILTER_COUNT = 128\n",
    "\n",
    "CHAR_FILTERS_LEN = [2, 3, 4]\n",
    "CHAR_FILTER_COUNT = 64\n",
    "CHAR_VECTOR_SIZE = 32\n",
    "\n",
    "POS_VECTOR_SIZE = 8\n",
    "\n",
    "ACTIVATION = \"relu\"\n",
    "PADDING = \"same\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 15, 10)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, 15, 10, 32)   4320        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_11 (TimeDistri (None, 15, 10, 64)   4160        time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 15, 10, 64)   6208        time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_17 (TimeDistri (None, 15, 10, 64)   8256        time_distributed_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_12 (TimeDistri (None, 15, 10, 64)   8256        time_distributed_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_15 (TimeDistri (None, 15, 10, 64)   12352       time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_18 (TimeDistri (None, 15, 10, 64)   16448       time_distributed_17[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 15, 300)      30140700    input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 15, 8)        152         input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 15, 10, 64)   256         time_distributed_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 15, 10, 64)   256         time_distributed_15[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 15, 10, 64)   256         time_distributed_18[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 15, 300)      0           embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 15, 8)        0           embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_13 (TimeDistri (None, 15, 64)       0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_16 (TimeDistri (None, 15, 64)       0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_19 (TimeDistri (None, 15, 64)       0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 15, 500)      0           dropout_2[0][0]                  \n",
      "                                                                 dropout_3[0][0]                  \n",
      "                                                                 time_distributed_13[0][0]        \n",
      "                                                                 time_distributed_16[0][0]        \n",
      "                                                                 time_distributed_19[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 15, 128)      128128      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 15, 128)      192128      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, 15, 128)      256128      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, 15, 128)      320128      concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 15, 128)      512         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 15, 128)      512         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 15, 128)      512         conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 15, 128)      512         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 128)          0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 512)          0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "                                                                 global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1572)         806436      concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 31,906,616\n",
      "Trainable params: 1,764,508\n",
      "Non-trainable params: 30,142,108\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(word_vocab_size, word_vector_size, word_embedding_matrix,\n",
    "                char_vocab_size, char_vector_size, \n",
    "                pos_vocab_size, pos_vector_size, output_size,\n",
    "                word_max_sequence_len, char_max_sequence_len,\n",
    "                word_filters_len, word_filter_count, \n",
    "                char_filters_len, char_filter_count,\n",
    "                activation=\"relu\", padding=\"same\"):\n",
    "\n",
    "    char_sequence_input = Input(shape=(word_max_sequence_len, char_max_sequence_len))\n",
    "    word_sequence_input = Input(shape=(word_max_sequence_len,))\n",
    "    pos_tag_sequence_input = Input(shape=(word_max_sequence_len,))\n",
    "    \n",
    "    char_embedded_sequences = TimeDistributed(\n",
    "        Embedding(\n",
    "            input_dim=char_vocab_size, \n",
    "            output_dim=char_vector_size,\n",
    "            embeddings_initializer=\"truncated_normal\",  # TODO: Change this?\n",
    "            trainable=True\n",
    "        ))(char_sequence_input)\n",
    "#     char_embedded_sequences = Dropout(\n",
    "#         rate=0.5,\n",
    "#         noise_shape=(None, word_max_sequence_len, 1, char_vector_size),\n",
    "#         seed=42\n",
    "#     )(char_embedded_sequences)\n",
    "\n",
    "    word_embedding_layer = Embedding(word_vocab_size, word_vector_size,\n",
    "                                     weights=[word_embedding_matrix],\n",
    "                                     input_length=word_max_sequence_len,\n",
    "                                     trainable=False)\n",
    "    word_embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "    word_embedded_sequences = Dropout(\n",
    "        rate=0.5,\n",
    "        noise_shape=(None, 1, word_vector_size),\n",
    "        seed=42\n",
    "    )(word_embedded_sequences)\n",
    "    \n",
    "    pos_tag_embedded_sequences = Embedding(pos_vocab_size, pos_vector_size,\n",
    "                                           embeddings_initializer=\"truncated_normal\",\n",
    "                                           trainable=True)(pos_tag_sequence_input)\n",
    "    pos_tag_embedded_sequences = Dropout(\n",
    "        rate=0.5,\n",
    "        noise_shape=(None, 1, pos_vector_size),\n",
    "        seed=42\n",
    "    )(pos_tag_embedded_sequences)\n",
    "    \n",
    "    char_layers = []\n",
    "    for filter_len in char_filters_len:\n",
    "        char_layer = TimeDistributed(\n",
    "            Conv1D(\n",
    "                char_filter_count,\n",
    "                filter_len,\n",
    "                padding=padding\n",
    "            )\n",
    "        )(char_embedded_sequences)\n",
    "        char_layer = TimeDistributed(\n",
    "            Conv1D(\n",
    "                char_filter_count, \n",
    "                filter_len, \n",
    "                padding=padding\n",
    "            )\n",
    "        )(char_layer)\n",
    "        char_layer = BatchNormalization(momentum=0.0)(char_layer)\n",
    "        char_layers.append(TimeDistributed(GlobalMaxPooling1D())(char_layer))\n",
    "    \n",
    "    word_layer = Concatenate()([word_embedded_sequences, pos_tag_embedded_sequences] + char_layers)\n",
    "\n",
    "    layers = []\n",
    "    for filter_len in word_filters_len:\n",
    "        layer = Conv1D(\n",
    "            word_filter_count,\n",
    "            filter_len,\n",
    "            activation=activation,\n",
    "            padding=padding\n",
    "        )(word_layer)\n",
    "        layer = BatchNormalization(momentum=0.0)(layer)\n",
    "        layers.append(GlobalMaxPooling1D()(layer))\n",
    "\n",
    "    layer = Concatenate()(layers)\n",
    "    preds = Dense(output_size, activation=\"softmax\")(layer)\n",
    "    model = Model(inputs=[word_sequence_input, pos_tag_sequence_input, char_sequence_input], outputs=[preds])\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "    word_vocab_size=len(word_index),\n",
    "    word_vector_size=w2v.vector_size, \n",
    "    word_embedding_matrix=word_embedding_matrix,\n",
    "    char_vocab_size=len(char_index),\n",
    "    char_vector_size=CHAR_VECTOR_SIZE,\n",
    "    pos_vocab_size=len(pos_tag_index),\n",
    "    pos_vector_size=POS_VECTOR_SIZE,\n",
    "    output_size=lbl_enc.classes_.shape[0],\n",
    "    word_max_sequence_len=WORD_MAX_SEQUENCE_LEN,\n",
    "    char_max_sequence_len=CHAR_MAX_SEQUENCE_LEN,\n",
    "    word_filters_len=WORD_FILTERS_LEN,\n",
    "    word_filter_count=WORD_FILTER_COUNT,\n",
    "    char_filters_len=CHAR_FILTERS_LEN,\n",
    "    char_filter_count=CHAR_FILTER_COUNT,\n",
    "    activation=ACTIVATION,\n",
    "    padding=PADDING\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"nadam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2211"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1369984 samples, validate on 499625 samples\n",
      "Epoch 1/10\n",
      "1369984/1369984 [==============================] - 332s 243us/sample - loss: 2.0819 - accuracy: 0.6546 - val_loss: 1.3529 - val_accuracy: 0.7470\n",
      "Epoch 2/10\n",
      "1369984/1369984 [==============================] - 318s 232us/sample - loss: 0.9183 - accuracy: 0.8204 - val_loss: 1.1163 - val_accuracy: 0.7865\n",
      "Epoch 3/10\n",
      "1369984/1369984 [==============================] - 317s 232us/sample - loss: 0.7682 - accuracy: 0.8439 - val_loss: 1.0486 - val_accuracy: 0.7979\n",
      "Epoch 4/10\n",
      " 933888/1369984 [===================>..........] - ETA: 1:30 - loss: 0.6806 - accuracy: 0.8579"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-5048246848a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mdev_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     ),\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    413\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=(train_word_sequences, train_pos_tag_sequences, train_char_sequences),\n",
    "    y=train_target,\n",
    "    batch_size=4096,\n",
    "    epochs=10,\n",
    "    validation_data=(\n",
    "        (dev_word_sequences, dev_pos_tag_sequences, dev_char_sequences),\n",
    "        dev_target\n",
    "    ),\n",
    "    validation_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1369986 samples, validate on 499625 samples\n",
      "Epoch 1/10\n",
      "1369986/1369986 [==============================] - 161s 117us/sample - loss: 2.1493 - accuracy: 0.6448 - val_loss: 1.4322 - val_accuracy: 0.7314\n",
      "Epoch 2/10\n",
      "1369986/1369986 [==============================] - 153s 111us/sample - loss: 0.9408 - accuracy: 0.8156 - val_loss: 1.1674 - val_accuracy: 0.7768\n",
      "Epoch 3/10\n",
      "1369986/1369986 [==============================] - 153s 111us/sample - loss: 0.7853 - accuracy: 0.8406 - val_loss: 1.0344 - val_accuracy: 0.8002\n",
      "Epoch 4/10\n",
      "1369986/1369986 [==============================] - 152s 111us/sample - loss: 0.7001 - accuracy: 0.8541 - val_loss: 1.0062 - val_accuracy: 0.8055\n",
      "Epoch 5/10\n",
      "1369986/1369986 [==============================] - 152s 111us/sample - loss: 0.6433 - accuracy: 0.8635 - val_loss: 0.9882 - val_accuracy: 0.8099\n",
      "Epoch 6/10\n",
      "1369986/1369986 [==============================] - 154s 112us/sample - loss: 0.6000 - accuracy: 0.8702 - val_loss: 0.9720 - val_accuracy: 0.8132\n",
      "Epoch 7/10\n",
      "1369986/1369986 [==============================] - 154s 112us/sample - loss: 0.5646 - accuracy: 0.8759 - val_loss: 0.9786 - val_accuracy: 0.8137\n",
      "Epoch 8/10\n",
      "1369986/1369986 [==============================] - 153s 112us/sample - loss: 0.5347 - accuracy: 0.8809 - val_loss: 0.9675 - val_accuracy: 0.8171\n",
      "Epoch 9/10\n",
      "1369986/1369986 [==============================] - 154s 112us/sample - loss: 0.5094 - accuracy: 0.8851 - val_loss: 0.9698 - val_accuracy: 0.8182\n",
      "Epoch 10/10\n",
      "1369986/1369986 [==============================] - 154s 112us/sample - loss: 0.4880 - accuracy: 0.8886 - val_loss: 0.9764 - val_accuracy: 0.8173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f40823f4588>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=(train_word_sequences, train_char_sequences),\n",
    "    y=train_target,\n",
    "    batch_size=4096,\n",
    "    epochs=10,\n",
    "    validation_data=(\n",
    "        (dev_word_sequences, dev_char_sequences),\n",
    "        dev_target\n",
    "    ),\n",
    "    validation_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4983500 samples, validate on 499625 samples\n",
      "Epoch 1/10\n",
      "4983500/4983500 [==============================] - 826s 166us/sample - loss: 1.4143 - accuracy: 0.7400 - val_loss: 0.9391 - val_accuracy: 0.8126\n",
      "Epoch 2/10\n",
      "4983500/4983500 [==============================] - 810s 162us/sample - loss: 0.8192 - accuracy: 0.8317 - val_loss: 0.8255 - val_accuracy: 0.8322\n",
      "Epoch 3/10\n",
      "4983500/4983500 [==============================] - 812s 163us/sample - loss: 0.7363 - accuracy: 0.8456 - val_loss: 0.7790 - val_accuracy: 0.8409\n",
      "Epoch 4/10\n",
      "4983500/4983500 [==============================] - 821s 165us/sample - loss: 0.6904 - accuracy: 0.8531 - val_loss: 0.7534 - val_accuracy: 0.8455\n",
      "Epoch 5/10\n",
      "4983500/4983500 [==============================] - 823s 165us/sample - loss: 0.6593 - accuracy: 0.8583 - val_loss: 0.7437 - val_accuracy: 0.8471\n",
      "Epoch 6/10\n",
      "4983500/4983500 [==============================] - 826s 166us/sample - loss: 0.6348 - accuracy: 0.8624 - val_loss: 0.7308 - val_accuracy: 0.8488\n",
      "Epoch 7/10\n",
      "4983500/4983500 [==============================] - 825s 165us/sample - loss: 0.6144 - accuracy: 0.8657 - val_loss: 0.7253 - val_accuracy: 0.8505\n",
      "Epoch 8/10\n",
      "4983500/4983500 [==============================] - 828s 166us/sample - loss: 0.5974 - accuracy: 0.8686 - val_loss: 0.7224 - val_accuracy: 0.8513\n",
      "Epoch 9/10\n",
      "4983500/4983500 [==============================] - 817s 164us/sample - loss: 0.5826 - accuracy: 0.8711 - val_loss: 0.7190 - val_accuracy: 0.8521\n",
      "Epoch 10/10\n",
      "4983500/4983500 [==============================] - 825s 165us/sample - loss: 0.5694 - accuracy: 0.8732 - val_loss: 0.7182 - val_accuracy: 0.8522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdd30be7dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=(train_word_sequences, train_char_sequences),\n",
    "    y=train_target,\n",
    "    batch_size=4096,\n",
    "    epochs=10,\n",
    "    validation_data=(\n",
    "        (dev_word_sequences, dev_char_sequences),\n",
    "        dev_target\n",
    "    ),\n",
    "    validation_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"dev\"][\"predictions\"] = model.predict(\n",
    "    (dev_word_sequences, dev_char_sequences), batch_size=1024, verbose=0\n",
    ").argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(datasets[\"dev\"][\"target\"], datasets[\"dev\"][\"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(datasets[\"dev\"][\"target\"], datasets[\"dev\"][\"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4983500 samples, validate on 499625 samples\n",
      "Epoch 1/10\n",
      "4983500/4983500 [==============================] - 826s 166us/sample - loss: 1.4143 - accuracy: 0.7400 - val_loss: 0.9391 - val_accuracy: 0.8126\n",
      "Epoch 2/10\n",
      "4983500/4983500 [==============================] - 810s 162us/sample - loss: 0.8192 - accuracy: 0.8317 - val_loss: 0.8255 - val_accuracy: 0.8322\n",
      "Epoch 3/10\n",
      "4983500/4983500 [==============================] - 812s 163us/sample - loss: 0.7363 - accuracy: 0.8456 - val_loss: 0.7790 - val_accuracy: 0.8409\n",
      "Epoch 4/10\n",
      "4983500/4983500 [==============================] - 821s 165us/sample - loss: 0.6904 - accuracy: 0.8531 - val_loss: 0.7534 - val_accuracy: 0.8455\n",
      "Epoch 5/10\n",
      "4983500/4983500 [==============================] - 823s 165us/sample - loss: 0.6593 - accuracy: 0.8583 - val_loss: 0.7437 - val_accuracy: 0.8471\n",
      "Epoch 6/10\n",
      "4983500/4983500 [==============================] - 826s 166us/sample - loss: 0.6348 - accuracy: 0.8624 - val_loss: 0.7308 - val_accuracy: 0.8488\n",
      "Epoch 7/10\n",
      "4983500/4983500 [==============================] - 825s 165us/sample - loss: 0.6144 - accuracy: 0.8657 - val_loss: 0.7253 - val_accuracy: 0.8505\n",
      "Epoch 8/10\n",
      "4983500/4983500 [==============================] - 828s 166us/sample - loss: 0.5974 - accuracy: 0.8686 - val_loss: 0.7224 - val_accuracy: 0.8513\n",
      "Epoch 9/10\n",
      "4983500/4983500 [==============================] - 817s 164us/sample - loss: 0.5826 - accuracy: 0.8711 - val_loss: 0.7190 - val_accuracy: 0.8521\n",
      "Epoch 10/10\n",
      "4983500/4983500 [==============================] - 825s 165us/sample - loss: 0.5694 - accuracy: 0.8732 - val_loss: 0.7182 - val_accuracy: 0.8522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdd30be7dd8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=(train_word_sequences, train_char_sequences),\n",
    "    y=train_target,\n",
    "    batch_size=4096,\n",
    "    epochs=10,\n",
    "    validation_data=(\n",
    "        (dev_word_sequences, dev_char_sequences),\n",
    "        dev_target\n",
    "    ),\n",
    "    validation_freq=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
