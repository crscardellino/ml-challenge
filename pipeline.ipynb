{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from os import path\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from string import punctuation\n",
    "from tensorflow.keras.layers import Concatenate, Conv1D, Dense, Embedding, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tqdm import tqdm_notebook\n",
    "from unidecode import unidecode\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"../data/\"\n",
    "LANGUAGE = \"spanish\"\n",
    "DROP_COLUMNS = [\"label_quality\", \"split\", \"language\", \"words\", \"pos\"]\n",
    "UNRELIABLE_SAMPLING = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38de5fc5841f4a75a69b9211cb2a0d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 1min 23s, sys: 21.2 s, total: 1min 45s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def load_data(base_path, language, drop_columns, unreliable_sampling):\n",
    "    datasets = {}\n",
    "    for ds in tqdm_notebook([\"train_reliable\", \"train_unreliable\", \"dev\", \"test\"]):\n",
    "        if ds == \"train_unreliable\" and unreliable_sampling == 0:\n",
    "            continue\n",
    "        \n",
    "        df = pd.read_parquet(\n",
    "            path.join(base_path, f\"{language}\", f\"{ds}.parquet\")\n",
    "        ).drop(drop_columns, axis=1, errors=\"ignore\")\n",
    "        \n",
    "        if ds == \"train_unreliable\" and 0 < unreliable_sampling < 1:\n",
    "            df = df.groupby([\"category\"]).apply(\n",
    "                lambda cat: cat.sample(frac=unreliable_sampling)\n",
    "            ).reset_index(drop=True)\n",
    "        elif ds == \"train_unreliable\" and unreliable_sampling > 1:\n",
    "            df = df.groupby([\"category\"]).apply(\n",
    "                lambda cat: cat.sample(n=int(unreliable_sampling))\n",
    "            ).reset_index(drop=True)\n",
    "        \n",
    "        if ds == \"train_reliable\":\n",
    "            datasets[\"train\"] = df\n",
    "        elif ds == \"train_unreliable\":\n",
    "            datasets[\"train\"] = pd.concat([\n",
    "                datasets[\"train\"],\n",
    "                df\n",
    "            ], ignore_index=True)\n",
    "        else:\n",
    "            datasets[ds] = df\n",
    "    \n",
    "    w2v = KeyedVectors.load_word2vec_format(\n",
    "        path.join(base_path, f\"{language}\", \"word2vec.bin.gz\"), \n",
    "        binary=True\n",
    "    )\n",
    "    \n",
    "    return datasets, w2v\n",
    "\n",
    "datasets, w2v = load_data(DIR_PATH, LANGUAGE, DROP_COLUMNS, UNRELIABLE_SAMPLING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Play Station 2 + Volante Hooligans.</td>\n",
       "      <td>GAME_CONSOLES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pilas Energizer Max Aa X1 - Tira X 20 Pilas</td>\n",
       "      <td>CELL_BATTERIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afeitadora Electrica Philips Hq6904 + Envio Gr...</td>\n",
       "      <td>SHAVING_MACHINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Estufa Calefactor Volcan 2500 Kcal/h 42512v Si...</td>\n",
       "      <td>HOME_HEATERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reloj Pared Vox Tronic Blanco Numeros 23cm Gar...</td>\n",
       "      <td>WALL_CLOCKS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title          category\n",
       "0               Play Station 2 + Volante Hooligans.      GAME_CONSOLES\n",
       "1        Pilas Energizer Max Aa X1 - Tira X 20 Pilas    CELL_BATTERIES\n",
       "2  Afeitadora Electrica Philips Hq6904 + Envio Gr...  SHAVING_MACHINES\n",
       "3  Estufa Calefactor Volcan 2500 Kcal/h 42512v Si...      HOME_HEATERS\n",
       "4  Reloj Pared Vox Tronic Blanco Numeros 23cm Gar...       WALL_CLOCKS"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rosario Contador De Billetes Uv / Mg Detecta F...</td>\n",
       "      <td>BILL_COUNTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Portón De Chapa 3 Hojas Mtr 2.50 Sin Marco</td>\n",
       "      <td>GARAGE_DOORS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Base Simil Cemento - 30 Cm X 5 Mm</td>\n",
       "      <td>CAKE_TOPPERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Disfraz De General Grievous Para Adultos, Tall...</td>\n",
       "      <td>COSTUMES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hermoso Árbol De Navidad En Madera De Pino</td>\n",
       "      <td>CHRISTMAS_TREES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title         category\n",
       "0  Rosario Contador De Billetes Uv / Mg Detecta F...    BILL_COUNTERS\n",
       "1         Portón De Chapa 3 Hojas Mtr 2.50 Sin Marco     GARAGE_DOORS\n",
       "2                  Base Simil Cemento - 30 Cm X 5 Mm     CAKE_TOPPERS\n",
       "3  Disfraz De General Grievous Para Adultos, Tall...         COSTUMES\n",
       "4         Hermoso Árbol De Navidad En Madera De Pino  CHRISTMAS_TREES"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"dev\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>Disco Rigido Externo Western Digital Elements ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>Picadora De Carne Fineschi Legitima 32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>Set Barreta Automotor Bremen X3 Unid. 6756 20 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>Miel Organica X250gr. (sin Tacc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>Bandeja Giradiscos Omnitronic Bd1320</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title\n",
       "0   9  Disco Rigido Externo Western Digital Elements ...\n",
       "1  10             Picadora De Carne Fineschi Legitima 32\n",
       "2  14  Set Barreta Automotor Bremen X3 Unid. 6756 20 ...\n",
       "3  15                   Miel Organica X250gr. (sin Tacc)\n",
       "4  19               Bandeja Giradiscos Omnitronic Bd1320"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"test\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LEN = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.99 s, sys: 1.08 s, total: 11.1 s\n",
      "Wall time: 4.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def label_encoder(*dfs):\n",
    "    labels = pd.concat(dfs)[\"category\"].tolist()\n",
    "    lbl_enc = LabelEncoder().fit(labels)\n",
    "\n",
    "    return lbl_enc\n",
    "\n",
    "lbl_enc = label_encoder(datasets[\"train\"], datasets[\"dev\"])\n",
    "\n",
    "for split in [\"train\", \"dev\"]:\n",
    "    datasets[split][\"target\"] = lbl_enc.transform(datasets[split][\"category\"])\n",
    "    datasets[split].drop([\"category\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text curation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def lowercase_titles(datasets):\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        datasets[split][\"title\"] = datasets[split][\"title\"].str.lower()\n",
    "    return datasets\n",
    "\n",
    "datasets = lowercase_titles(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa07e2b12864c6ba2e8edacdfa63dfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 5min 47s, sys: 5.89 s, total: 5min 53s\n",
      "Wall time: 5min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def tokenization(datasets, language):\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        datasets[split][\"title\"] = datasets[split][\"title\"].apply(\n",
    "            lambda title: word_tokenize(title, language=language)\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "datasets = tokenization(datasets, LANGUAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CPU times: user 6.7 s, sys: 425 ms, total: 7.12 s\n",
      "Wall time: 7.11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def remove_punctuation(datasets, punctuation):\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        datasets[split][\"title\"] = datasets[split][\"title\"].apply(\n",
    "            lambda words: [w for w in words if w not in punctuation]\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "datasets = remove_punctuation(datasets, punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d4667d6b6e4e7596b70e7ddc6fe75e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.52 s, sys: 164 ms, total: 8.69 s\n",
      "Wall time: 8.66 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def remove_stopwords(datasets, stopwords):\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        datasets[split][\"title\"] = datasets[split][\"title\"].apply(\n",
    "            lambda words: [w for w in words if w not in stopwords]\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "datasets = remove_stopwords(datasets, set(stopwords.words(LANGUAGE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3435c5f1652d46b2ae0393421ce0c7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 38s, sys: 819 ms, total: 2min 39s\n",
      "Wall time: 2min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def word_with_vector(word, w2v, stemmer):\n",
    "    if word in w2v:\n",
    "        return word\n",
    "    elif word.capitalize() in w2v:\n",
    "        return word.capitalize()\n",
    "    elif word.upper() in w2v:\n",
    "        return word.upper()\n",
    "    elif unidecode(word) in w2v:\n",
    "        return unidecode(word)\n",
    "    elif unidecode(word.capitalize()) in w2v:\n",
    "        return unidecode(word.capitalize())\n",
    "    elif unidecode(word.upper()) in w2v:\n",
    "        return unidecode(word.upper())\n",
    "    elif stemmer.stem(word) in w2v:\n",
    "        return stemmer.stem(word)\n",
    "    elif re.search(\"\\d+\", word):\n",
    "        return \"<NUM>\"\n",
    "    else:\n",
    "        return \"<UNK>\"\n",
    "    # TODO: Lemmatization? Other normalizations?\n",
    "\n",
    "def word_vectorize(datasets, language, w2v):\n",
    "    stemmer = SnowballStemmer(language)\n",
    "    for split in tqdm_notebook(datasets):\n",
    "        datasets[split][\"title\"] = datasets[split][\"title\"].apply(\n",
    "            lambda words: [word_with_vector(w, w2v, stemmer) for w in words]\n",
    "        )\n",
    "    return datasets\n",
    "\n",
    "datasets = word_vectorize(datasets, LANGUAGE, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 121998\n",
      "CPU times: user 11.3 s, sys: 476 ms, total: 11.8 s\n",
      "Wall time: 7.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def words_to_idx(all_words, w2v, null_token=\"<NULL>\",\n",
    "                 unknown_token=\"<UNK>\", num_token=\"<NUM>\"):\n",
    "    word_index = {word for words in all_words for word in words if word in w2v}\n",
    "    word_index = {word: idx for idx, word in enumerate(sorted(word_index), start=1)}\n",
    "    word_index[null_token] = 0\n",
    "    word_index[num_token] = len(word_index)\n",
    "    word_index[unknown_token] = len(word_index)\n",
    "\n",
    "    return word_index\n",
    "\n",
    "word_index = words_to_idx(pd.concat(list(datasets.values()), sort=False)[\"title\"], w2v)\n",
    "\n",
    "print(f\"Vocab length: {len(word_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.9 s, sys: 264 ms, total: 25.1 s\n",
      "Wall time: 25.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def sequence_padding(series, word_index, max_len):\n",
    "    return pad_sequences(\n",
    "            series.apply(\n",
    "                lambda words: [word_index.get(word, word_index[\"<UNK>\"]) for word in words]\n",
    "            ).tolist(), maxlen=max_len\n",
    "        )\n",
    "\n",
    "train_word_sequences = sequence_padding(\n",
    "    datasets[\"train\"][\"title\"], word_index, MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "dev_word_sequences = sequence_padding(\n",
    "    datasets[\"dev\"][\"title\"], word_index, MAX_SEQUENCE_LEN\n",
    ")\n",
    "\n",
    "test_word_sequences = sequence_padding(\n",
    "    datasets[\"test\"][\"title\"], word_index, MAX_SEQUENCE_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 563 ms, sys: 3.84 s, total: 4.4 s\n",
      "Wall time: 4.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = to_categorical(\n",
    "    datasets[\"train\"][\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")\n",
    "\n",
    "dev_target = to_categorical(\n",
    "    datasets[\"dev\"][\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 474 ms, sys: 116 ms, total: 590 ms\n",
      "Wall time: 588 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_embedding_matrix(word_index, w2v):\n",
    "    embedding_matrix = np.zeros((len(word_index), w2v.vector_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in w2v and word not in {\"<NULL>\", \"<UNK>\", \"<NUM>\"}:\n",
    "            embedding_matrix[i] = w2v[word]\n",
    "        elif word == \"<UNK>\" or word == \"<NUM>\":\n",
    "            embedding_matrix[i] = np.random.normal(size=(w2v.vector_size,))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "word_embedding_matrix = get_embedding_matrix(word_index, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTERS = [2, 3, 4, 5]\n",
    "FILTER_COUNT = 128\n",
    "ACTIVATION = \"relu\"\n",
    "PADDING = \"valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 10)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 300)      36599400    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 9, 128)       76928       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 8, 128)       115328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 7, 128)       153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 6, 128)       192128      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 128)          0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 128)          0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_7 (GlobalM (None, 128)          0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_8 (GlobalM (None, 128)          0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 global_max_pooling1d_7[0][0]     \n",
      "                                                                 global_max_pooling1d_8[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1573)         806949      concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 37,944,461\n",
      "Trainable params: 1,345,061\n",
      "Non-trainable params: 36,599,400\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(word_vocab_size, word_vector_size, word_embedding_matrix, output_size, max_sequence_len,\n",
    "              filters, filter_count, activation=\"relu\", padding=\"valid\"):\n",
    "    word_embedding_layer = Embedding(word_vocab_size, word_vector_size,\n",
    "                                     weights=[word_embedding_matrix],\n",
    "                                     input_length=max_sequence_len,\n",
    "                                     trainable=False)\n",
    "\n",
    "    word_sequence_input = Input(shape=(max_sequence_len,))\n",
    "    word_embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "\n",
    "    layers = []\n",
    "    for filter_size in filters:\n",
    "        layer = Conv1D(\n",
    "            filter_count,\n",
    "            filter_size,\n",
    "            activation=activation,\n",
    "            padding=padding,\n",
    "            kernel_regularizer=l2(0.01)\n",
    "        )(word_embedded_sequences)\n",
    "        layer = GlobalMaxPooling1D()(layer)\n",
    "        layers.append(layer)\n",
    "\n",
    "    layer = Concatenate()(layers)\n",
    "    preds = Dense(output_size, activation=\"softmax\")(layer)\n",
    "    model = Model(word_sequence_input, preds)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_cnn(\n",
    "    word_vocab_size=len(word_index),\n",
    "    word_vector_size=w2v.vector_size, \n",
    "    word_embedding_matrix=word_embedding_matrix,\n",
    "    output_size=lbl_enc.classes_.shape[0],\n",
    "    max_sequence_len=MAX_SEQUENCE_LEN,\n",
    "    filters=FILTERS,\n",
    "    filter_count=FILTER_COUNT,\n",
    "    activation=ACTIVATION,\n",
    "    padding=PADDING\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"nadam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2725070 samples, validate on 499625 samples\n",
      "Epoch 1/10\n",
      "2725070/2725070 [==============================] - 309s 113us/sample - loss: 3.9623 - accuracy: 0.4600\n",
      "Epoch 2/10\n",
      "2725070/2725070 [==============================] - 303s 111us/sample - loss: 2.5736 - accuracy: 0.6418\n",
      "Epoch 3/10\n",
      "2725070/2725070 [==============================] - 379s 139us/sample - loss: 2.2472 - accuracy: 0.6791\n",
      "Epoch 4/10\n",
      "2725070/2725070 [==============================] - 454s 167us/sample - loss: 2.0761 - accuracy: 0.6982\n",
      "Epoch 5/10\n",
      "2725070/2725070 [==============================] - 544s 200us/sample - loss: 1.9701 - accuracy: 0.7103 - val_loss: 2.0630 - val_accuracy: 0.6908\n",
      "Epoch 6/10\n",
      "2725070/2725070 [==============================] - 453s 166us/sample - loss: 1.8971 - accuracy: 0.7181\n",
      "Epoch 7/10\n",
      "2725070/2725070 [==============================] - 462s 169us/sample - loss: 1.8429 - accuracy: 0.7241\n",
      "Epoch 8/10\n",
      "1933312/2725070 [====================>.........] - ETA: 2:15 - loss: 1.8055 - accuracy: 0.7280"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-40014342b85d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_word_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    413\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1820\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1824\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/.local/conda/envs/nlu/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=train_word_sequences,\n",
    "    y=train_target,\n",
    "    batch_size=1024,\n",
    "    epochs=10,\n",
    "    validation_data=(dev_word_sequences, dev_target),\n",
    "    validation_freq=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"dev\"][\"predictions\"] = model.predict(\n",
    "    dev_word_sequences, batch_size=1024, verbose=0\n",
    ").argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ccardellino/.local/conda/envs/nlu/lib/python3.7/site-packages/sklearn/metrics/classification.py:1745: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7298081600204341"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(datasets[\"dev\"][\"target\"], datasets[\"dev\"][\"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
