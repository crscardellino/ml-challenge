{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Concatenate, Conv1D, Dense, Embedding, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from unidecode import unidecode\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, language, columns, sample=0):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df[\"words\"] = df[\"words\"].apply(lambda words: [unidecode(w) for w in words])\n",
    "    if sample == 0:\n",
    "        return df[df[\"language\"] == language][columns]\n",
    "    else:\n",
    "        return df[df[\"language\"] == language].groupby(\"category\").apply(\n",
    "            lambda cat: cat.sample(frac=sample)\n",
    "        ).reset_index(drop=True)[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"../data/meli/train_reliable.parquet\"\n",
    "trainu_data = \"../data/meli/train_unreliable.parquet\"\n",
    "dev_data = \"../data/meli/dev.parquet\"\n",
    "test_data = \"../data/meli/test.parquet\"\n",
    "language = \"spanish\"\n",
    "word_vectors = f\"../data/{language}/{language}-word2vec.bin.gz\"\n",
    "columns = [\"words\", \"pos\", \"category\"]\n",
    "max_sequence_len = 20\n",
    "pos_vector_size = 3\n",
    "filters = [2, 3, 5]\n",
    "filter_count = 256\n",
    "network_size = 0\n",
    "layer_size = 128\n",
    "activation = \"relu\"\n",
    "padding = \"valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_df = pd.concat([\n",
    "    load_data(train_data, language, columns), \n",
    "    load_data(trainu_data, language, columns, 0.25)\n",
    "])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dev_df = pd.concat([\n",
    "    load_data(dev_data, language, columns)\n",
    "])\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "test_df = pd.concat([\n",
    "    load_data(test_data, language, [\"id\", \"words\", \"pos\"])\n",
    "])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "w2v = KeyedVectors.load_word2vec_format(word_vectors, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def label_encoder(*dfs):\n",
    "    labels = pd.concat(dfs)[\"category\"].tolist()\n",
    "    lbl_enc = LabelEncoder().fit(labels)\n",
    "\n",
    "    return lbl_enc\n",
    "\n",
    "lbl_enc = label_encoder(train_df, dev_df)\n",
    "\n",
    "train_df[\"target\"] = lbl_enc.transform(train_df[\"category\"])\n",
    "train_df.drop([\"category\"], axis=1, inplace=True)\n",
    "\n",
    "dev_df[\"target\"] = lbl_enc.transform(dev_df[\"category\"])\n",
    "dev_df.drop([\"category\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words/PoS to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def words_to_idx(all_words, w2v, null_token=\"<NULL>\", unknown_token=\"<UNK>\"):\n",
    "    word_index = {word for words in all_words for word in words if word in w2v}\n",
    "    word_index = {word: idx for idx, word in enumerate(sorted(word_index), start=1)}\n",
    "    word_index[null_token] = 0\n",
    "    word_index[unknown_token] = len(word_index)\n",
    "\n",
    "    return word_index\n",
    "\n",
    "def pos_to_idx(all_pos, null_pos=\"<NULL>\"):\n",
    "    pos_index = {pos for pos_tags in all_pos for pos in pos_tags}\n",
    "    pos_index = {pos: idx for idx, pos in enumerate(sorted(pos_index), start=1)}\n",
    "    pos_index[null_pos] = 0\n",
    "\n",
    "    return pos_index\n",
    "\n",
    "word_index = words_to_idx(pd.concat([train_df, dev_df, test_df], sort=False)[\"words\"], w2v)\n",
    "pos_index = pos_to_idx(pd.concat([train_df, dev_df, test_df], sort=False)[\"pos\"])\n",
    "\n",
    "print(f\"Vocab length: {len(word_index)} - PoS length: {len(pos_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def sequence_padding(df, word_index, pos_index, max_len):\n",
    "    return (\n",
    "        pad_sequences(\n",
    "            df[\"words\"].apply(\n",
    "                lambda words: [word_index.get(word, word_index[\"<UNK>\"]) for word in words]\n",
    "            ).tolist(), maxlen=max_len\n",
    "        ),\n",
    "        pad_sequences(\n",
    "            df[\"pos\"].apply(\n",
    "                lambda pos: [pos_index.get(p) for p in pos]\n",
    "            ).tolist(), maxlen=max_len\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_word_sequences, train_pos_sequences = sequence_padding(\n",
    "    train_df, word_index, pos_index, max_sequence_len\n",
    ")\n",
    "\n",
    "dev_word_sequences, dev_pos_sequences = sequence_padding(\n",
    "    dev_df, word_index, pos_index, max_sequence_len\n",
    ")\n",
    "\n",
    "test_word_sequences, test_pos_sequences = sequence_padding(\n",
    "    test_df, word_index, pos_index, max_sequence_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_target = to_categorical(\n",
    "    train_df[\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")\n",
    "\n",
    "dev_target = to_categorical(\n",
    "    dev_df[\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def get_embedding_matrix(word_index, w2v):\n",
    "    embedding_matrix = np.zeros((len(word_index), w2v.vector_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in w2v and word not in {\"<NULL>\", \"<UNK>\"}:\n",
    "            embedding_matrix[i] = w2v[word]\n",
    "        if word == \"<UNK>\":\n",
    "            embedding_matrix[i] = np.random.normal(size=(w2v.vector_size,))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "word_embedding_matrix = get_embedding_matrix(word_index, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn(word_vocab_size, word_vector_size, word_embedding_matrix, \n",
    "              pos_vocab_size, pos_vector_size, output_size, max_sequence_len,\n",
    "              filters, filter_count, network_size, layer_size,\n",
    "              activation=\"relu\", padding=\"valid\"):\n",
    "    word_embedding_layer = Embedding(word_vocab_size, word_vector_size,\n",
    "                                     weights=[word_embedding_matrix],\n",
    "                                     input_length=max_sequence_len,\n",
    "                                     trainable=False)\n",
    "    pos_embedding_layer = Embedding(pos_vocab_size, pos_vector_size,\n",
    "                                    embeddings_initializer=\"truncated_normal\",\n",
    "                                    input_length=max_sequence_len)\n",
    "\n",
    "    word_sequence_input = Input(shape=(max_sequence_len,))\n",
    "    word_embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "\n",
    "    pos_sequence_input = Input(shape=(max_sequence_len,))\n",
    "    pos_embedded_sequences = pos_embedding_layer(pos_sequence_input)\n",
    "    \n",
    "    embedded_sequences = Concatenate()([word_embedded_sequences, pos_embedded_sequences])\n",
    "\n",
    "    layers = []\n",
    "    for filter_size in filters:\n",
    "        layer = Conv1D(\n",
    "            filter_count,\n",
    "            filter_size,\n",
    "            activation=activation,\n",
    "            padding=padding\n",
    "        )(embedded_sequences)\n",
    "        layer = GlobalMaxPooling1D()(layer)\n",
    "        layers.append(layer)\n",
    "\n",
    "    layer = Concatenate()(layers)    \n",
    "\n",
    "    for _ in range(network_size):\n",
    "        layer = Dense(layer_size, activation=activation)(layer)\n",
    "\n",
    "    preds = Dense(output_size, activation=\"softmax\")(layer)\n",
    "    model = Model([word_sequence_input, pos_sequence_input], preds)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_cnn(\n",
    "    word_vocab_size=len(word_index),\n",
    "    word_vector_size=w2v.vector_size, \n",
    "    word_embedding_matrix=word_embedding_matrix,\n",
    "    pos_vocab_size=len(pos_index),\n",
    "    pos_vector_size=pos_vector_size,\n",
    "    output_size=lbl_enc.classes_.shape[0],\n",
    "    max_sequence_len=max_sequence_len,\n",
    "    filters=filters,\n",
    "    filter_count=filter_count,\n",
    "    network_size=network_size,\n",
    "    layer_size=layer_size,\n",
    "    activation=activation,\n",
    "    padding=padding\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"nadam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x=(train_word_sequences, train_pos_sequences),\n",
    "    y=train_target,\n",
    "    batch_size=1024,\n",
    "    epochs=10,\n",
    "    validation_data=(\n",
    "        (dev_word_sequences, dev_pos_sequences),\n",
    "        dev_target\n",
    "    ),\n",
    "    validation_freq=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df[\"predictions\"] = model.predict(\n",
    "    (dev_word_sequences, dev_pos_sequences), batch_size=1024, verbose=0\n",
    ").argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_accuracy_score(dev_df[\"target\"], dev_df[\"predictions\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
