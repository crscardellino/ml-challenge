{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Concatenate, Conv1D, Dense, Embedding, GlobalMaxPooling1D, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from unidecode import unidecode\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.compat.v1.random.set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path, language, columns, sample=0):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    if sample == 0:\n",
    "        return df[df[\"language\"] == language][columns]\n",
    "    else:\n",
    "        return df[df[\"language\"] == language].groupby(\"category\").apply(\n",
    "            lambda cat: cat.sample(frac=sample)\n",
    "        ).reset_index(drop=True)[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = \"../data/meli/train_reliable.parquet\"\n",
    "trainu_data = \"../data/meli/train_unreliable.parquet\"\n",
    "dev_data = \"../data/meli/dev.parquet\"\n",
    "test_data = \"../data/meli/test.parquet\"\n",
    "language = \"spanish\"\n",
    "word_vectors = f\"../data/{language}/{language}-word2vec.bin.gz\"\n",
    "columns = [\"words\", \"pos\", \"category\"]\n",
    "max_sequence_len = 20\n",
    "pos_vector_size = 3\n",
    "filters = [2, 3, 5, 10]\n",
    "filter_count = 128\n",
    "network_size = 1\n",
    "layer_size = 128\n",
    "activation = \"relu\"\n",
    "padding = \"valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 35.1 s, total: 2min 2s\n",
      "Wall time: 1min 17s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>pos</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>[play, station, 2, +, volante, hooligans, .]</td>\n",
       "      <td>[VERB, NOUN, NUM, PROPN, ADV, ADJ, PUNCT]</td>\n",
       "      <td>GAME_CONSOLES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>[pilas, energizer, max, aa, x1, -, tira, x, 20...</td>\n",
       "      <td>[NOUN, VERB, ADP, DET, PROPN, PUNCT, VERB, CON...</td>\n",
       "      <td>CELL_BATTERIES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>[afeitadora, electrica, philips, hq6904, +, en...</td>\n",
       "      <td>[ADJ, ADJ, ADP, NOUN, PROPN, VERB, PROPN]</td>\n",
       "      <td>SHAVING_MACHINES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>[estufa, calefactor, volcan, 2500, kcal, /, h,...</td>\n",
       "      <td>[NOUN, NOUN, VERB, NOUN, NOUN, PUNCT, CONJ, NU...</td>\n",
       "      <td>HOME_HEATERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>449</th>\n",
       "      <td>[reloj, pared, vox, tronic, blanco, numeros, 2...</td>\n",
       "      <td>[NOUN, NOUN, NOUN, NOUN, ADJ, NOUN, NUM, ADP, ...</td>\n",
       "      <td>WALL_CLOCKS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 words  \\\n",
       "267       [play, station, 2, +, volante, hooligans, .]   \n",
       "272  [pilas, energizer, max, aa, x1, -, tira, x, 20...   \n",
       "287  [afeitadora, electrica, philips, hq6904, +, en...   \n",
       "386  [estufa, calefactor, volcan, 2500, kcal, /, h,...   \n",
       "449  [reloj, pared, vox, tronic, blanco, numeros, 2...   \n",
       "\n",
       "                                                   pos          category  \n",
       "267          [VERB, NOUN, NUM, PROPN, ADV, ADJ, PUNCT]     GAME_CONSOLES  \n",
       "272  [NOUN, VERB, ADP, DET, PROPN, PUNCT, VERB, CON...    CELL_BATTERIES  \n",
       "287          [ADJ, ADJ, ADP, NOUN, PROPN, VERB, PROPN]  SHAVING_MACHINES  \n",
       "386  [NOUN, NOUN, VERB, NOUN, NOUN, PUNCT, CONJ, NU...      HOME_HEATERS  \n",
       "449  [NOUN, NOUN, NOUN, NOUN, ADJ, NOUN, NUM, ADP, ...       WALL_CLOCKS  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.concat([\n",
    "    load_data(train_data, language, columns), \n",
    "    load_data(trainu_data, language, columns, 0.05)\n",
    "])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 10.3 s, total: 22.8 s\n",
      "Wall time: 13.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>pos</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[rosario, contador, de, billetes, uv, /, mg, d...</td>\n",
       "      <td>[ADJ, NOUN, ADP, NOUN, ADJ, PUNCT, ADV, VERB, ...</td>\n",
       "      <td>BILL_COUNTERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[port칩n, de, chapa, 3, hojas, mtr, 2.50, sin, ...</td>\n",
       "      <td>[NOUN, ADP, NOUN, NUM, NOUN, PRON, NUM, ADP, N...</td>\n",
       "      <td>GARAGE_DOORS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[base, simil, cemento, -, 30, cm, x, 5, mm]</td>\n",
       "      <td>[NOUN, VERB, NOUN, PUNCT, NUM, NOUN, CONJ, NUM...</td>\n",
       "      <td>CAKE_TOPPERS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[disfraz, de, general, grievous, para, adultos...</td>\n",
       "      <td>[NOUN, ADP, NOUN, ADJ, ADP, NOUN, PUNCT, NOUN,...</td>\n",
       "      <td>COSTUMES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>[hermoso, 치rbol, de, navidad, en, madera, de, ...</td>\n",
       "      <td>[ADJ, NOUN, ADP, NOUN, ADP, NOUN, ADP, NOUN]</td>\n",
       "      <td>CHRISTMAS_TREES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                words  \\\n",
       "11  [rosario, contador, de, billetes, uv, /, mg, d...   \n",
       "23  [port칩n, de, chapa, 3, hojas, mtr, 2.50, sin, ...   \n",
       "26        [base, simil, cemento, -, 30, cm, x, 5, mm]   \n",
       "34  [disfraz, de, general, grievous, para, adultos...   \n",
       "45  [hermoso, 치rbol, de, navidad, en, madera, de, ...   \n",
       "\n",
       "                                                  pos         category  \n",
       "11  [ADJ, NOUN, ADP, NOUN, ADJ, PUNCT, ADV, VERB, ...    BILL_COUNTERS  \n",
       "23  [NOUN, ADP, NOUN, NUM, NOUN, PRON, NUM, ADP, N...     GARAGE_DOORS  \n",
       "26  [NOUN, VERB, NOUN, PUNCT, NUM, NOUN, CONJ, NUM...     CAKE_TOPPERS  \n",
       "34  [NOUN, ADP, NOUN, ADJ, ADP, NOUN, PUNCT, NOUN,...         COSTUMES  \n",
       "45       [ADJ, NOUN, ADP, NOUN, ADP, NOUN, ADP, NOUN]  CHRISTMAS_TREES  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "dev_df = pd.concat([\n",
    "    load_data(dev_data, language, columns)\n",
    "])\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 983 ms, sys: 184 ms, total: 1.17 s\n",
      "Wall time: 965 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>words</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>[disco, rigido, externo, western, digital, ele...</td>\n",
       "      <td>[NOUN, ADJ, ADJ, NOUN, ADJ, NOUN, NUM, ADP, PR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>[picadora, de, carne, fineschi, legitima, 32]</td>\n",
       "      <td>[NOUN, ADP, NOUN, NOUN, ADJ, NUM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>[set, barreta, automotor, bremen, x3, unid, .,...</td>\n",
       "      <td>[NUM, NOUN, NOUN, ADJ, PROPN, VERB, PUNCT, NOU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>[miel, organica, x250gr, ., (, sin, tacc, )]</td>\n",
       "      <td>[NOUN, ADJ, PROPN, PUNCT, PUNCT, ADP, NOUN, PU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>[bandeja, giradiscos, omnitronic, bd1320]</td>\n",
       "      <td>[VERB, NOUN, VERB, ADJ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              words  \\\n",
       "9    9  [disco, rigido, externo, western, digital, ele...   \n",
       "10  10      [picadora, de, carne, fineschi, legitima, 32]   \n",
       "14  14  [set, barreta, automotor, bremen, x3, unid, .,...   \n",
       "15  15       [miel, organica, x250gr, ., (, sin, tacc, )]   \n",
       "19  19          [bandeja, giradiscos, omnitronic, bd1320]   \n",
       "\n",
       "                                                  pos  \n",
       "9   [NOUN, ADJ, ADJ, NOUN, ADJ, NOUN, NUM, ADP, PR...  \n",
       "10                  [NOUN, ADP, NOUN, NOUN, ADJ, NUM]  \n",
       "14  [NUM, NOUN, NOUN, ADJ, PROPN, VERB, PUNCT, NOU...  \n",
       "15  [NOUN, ADJ, PROPN, PUNCT, PUNCT, ADP, NOUN, PU...  \n",
       "19                            [VERB, NOUN, VERB, ADJ]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "test_df = pd.concat([\n",
    "    load_data(test_data, language, [\"id\", \"words\", \"pos\"])\n",
    "])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.1 s, sys: 828 ms, total: 34.9 s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "w2v = KeyedVectors.load_word2vec_format(word_vectors, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.51 s, sys: 1.04 s, total: 8.55 s\n",
      "Wall time: 2.25 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def label_encoder(*dfs):\n",
    "    labels = pd.concat(dfs)[\"category\"].tolist()\n",
    "    lbl_enc = LabelEncoder().fit(labels)\n",
    "\n",
    "    return lbl_enc\n",
    "\n",
    "lbl_enc = label_encoder(train_df, dev_df)\n",
    "\n",
    "train_df[\"target\"] = lbl_enc.transform(train_df[\"category\"])\n",
    "train_df.drop([\"category\"], axis=1, inplace=True)\n",
    "\n",
    "dev_df[\"target\"] = lbl_enc.transform(dev_df[\"category\"])\n",
    "dev_df.drop([\"category\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words/PoS to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab length: 63757 - PoS length: 18\n",
      "CPU times: user 14.3 s, sys: 469 ms, total: 14.8 s\n",
      "Wall time: 7.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def words_to_idx(all_words, w2v, null_token=\"<NULL>\", unknown_token=\"<UNK>\"):\n",
    "    word_index = {word for words in all_words for word in words if word in w2v}\n",
    "    word_index = {word: idx for idx, word in enumerate(sorted(word_index), start=1)}\n",
    "    word_index[null_token] = 0\n",
    "    word_index[unknown_token] = len(word_index)\n",
    "\n",
    "    return word_index\n",
    "\n",
    "def pos_to_idx(all_pos, null_pos=\"<NULL>\"):\n",
    "    pos_index = {pos for pos_tags in all_pos for pos in pos_tags}\n",
    "    pos_index = {pos: idx for idx, pos in enumerate(sorted(pos_index), start=1)}\n",
    "    pos_index[null_pos] = 0\n",
    "\n",
    "    return pos_index\n",
    "\n",
    "word_index = words_to_idx(pd.concat([train_df, dev_df, test_df], sort=False)[\"words\"], w2v)\n",
    "pos_index = pos_to_idx(pd.concat([train_df, dev_df, test_df], sort=False)[\"pos\"])\n",
    "\n",
    "print(f\"Vocab length: {len(word_index)} - PoS length: {len(pos_index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.3 s, sys: 113 ms, total: 22.4 s\n",
      "Wall time: 22.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def sequence_padding(df, word_index, pos_index, max_len):\n",
    "    return (\n",
    "        pad_sequences(\n",
    "            df[\"words\"].apply(\n",
    "                lambda words: [word_index.get(word, word_index[\"<UNK>\"]) for word in words]\n",
    "            ).tolist(), maxlen=max_len\n",
    "        ),\n",
    "        pad_sequences(\n",
    "            df[\"pos\"].apply(\n",
    "                lambda pos: [pos_index.get(p) for p in pos]\n",
    "            ).tolist(), maxlen=max_len\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_word_sequences, train_pos_sequences = sequence_padding(\n",
    "    train_df, word_index, pos_index, max_sequence_len\n",
    ")\n",
    "\n",
    "dev_word_sequences, dev_pos_sequences = sequence_padding(\n",
    "    dev_df, word_index, pos_index, max_sequence_len\n",
    ")\n",
    "\n",
    "test_word_sequences, test_pos_sequences = sequence_padding(\n",
    "    test_df, word_index, pos_index, max_sequence_len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 342 ms, sys: 1.7 s, total: 2.04 s\n",
      "Wall time: 2.04 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_target = to_categorical(\n",
    "    train_df[\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")\n",
    "\n",
    "dev_target = to_categorical(\n",
    "    dev_df[\"target\"].tolist(),\n",
    "    num_classes=lbl_enc.classes_.shape[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 245 ms, sys: 44.3 ms, total: 290 ms\n",
      "Wall time: 288 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_embedding_matrix(word_index, w2v):\n",
    "    embedding_matrix = np.zeros((len(word_index), w2v.vector_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if word in w2v and word not in {\"<NULL>\", \"<UNK>\"}:\n",
    "            embedding_matrix[i] = w2v[word]\n",
    "        if word == \"<UNK>\":\n",
    "            embedding_matrix[i] = np.random.normal(size=(w2v.vector_size,))\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "word_embedding_matrix = get_embedding_matrix(word_index, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_12 (InputLayer)           [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 20, 300)      19127100    input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_11 (Embedding)        (None, 20, 3)        54          input_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 20, 303)      0           embedding_10[0][0]               \n",
      "                                                                 embedding_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 19, 128)      77696       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 18, 128)      116480      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 16, 128)      194048      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 11, 128)      387968      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 128)          0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 128)          0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_14 (Global (None, 128)          0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_15 (Global (None, 128)          0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512)          0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "                                                                 global_max_pooling1d_14[0][0]    \n",
      "                                                                 global_max_pooling1d_15[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          65664       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1572)         202788      dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 20,171,798\n",
      "Trainable params: 1,044,698\n",
      "Non-trainable params: 19,127,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_cnn(word_vocab_size, word_vector_size, word_embedding_matrix, \n",
    "              pos_vocab_size, pos_vector_size, output_size, max_sequence_len,\n",
    "              filters, filter_count, network_size, layer_size,\n",
    "              activation=\"relu\", padding=\"valid\"):\n",
    "    word_embedding_layer = Embedding(word_vocab_size, word_vector_size,\n",
    "                                     weights=[word_embedding_matrix],\n",
    "                                     input_length=max_sequence_len,\n",
    "                                     trainable=False)\n",
    "    pos_embedding_layer = Embedding(pos_vocab_size, pos_vector_size,\n",
    "                                    embeddings_initializer=\"truncated_normal\",\n",
    "                                    input_length=max_sequence_len)\n",
    "\n",
    "    word_sequence_input = Input(shape=(max_sequence_len,))\n",
    "    word_embedded_sequences = word_embedding_layer(word_sequence_input)\n",
    "\n",
    "    pos_sequence_input = Input(shape=(max_sequence_len,))\n",
    "    pos_embedded_sequences = pos_embedding_layer(pos_sequence_input)\n",
    "    \n",
    "    embedded_sequences = Concatenate()([word_embedded_sequences, pos_embedded_sequences])\n",
    "\n",
    "    layers = []\n",
    "    for filter_size in filters:\n",
    "        layer = Conv1D(\n",
    "            filter_count,\n",
    "            filter_size,\n",
    "            activation=activation,\n",
    "            padding=padding\n",
    "        )(embedded_sequences)\n",
    "        layer = GlobalMaxPooling1D()(layer)\n",
    "        layers.append(layer)\n",
    "\n",
    "    layer = Concatenate()(layers)    \n",
    "\n",
    "    for _ in range(network_size):\n",
    "        layer = Dense(layer_size, activation=activation)(layer)\n",
    "\n",
    "    preds = Dense(output_size, activation=\"softmax\")(layer)\n",
    "    model = Model([word_sequence_input, pos_sequence_input], preds)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = build_cnn(\n",
    "    word_vocab_size=len(word_index),\n",
    "    word_vector_size=w2v.vector_size, \n",
    "    word_embedding_matrix=word_embedding_matrix,\n",
    "    pos_vocab_size=len(pos_index),\n",
    "    pos_vector_size=pos_vector_size,\n",
    "    output_size=lbl_enc.classes_.shape[0],\n",
    "    max_sequence_len=max_sequence_len,\n",
    "    filters=filters,\n",
    "    filter_count=filter_count,\n",
    "    network_size=network_size,\n",
    "    layer_size=layer_size,\n",
    "    activation=activation,\n",
    "    padding=padding\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=\"nadam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 918295 samples, validate on 499625 samples\n",
      "Epoch 1/10\n",
      "918295/918295 [==============================] - 89s 96us/sample - loss: 2.4571 - accuracy: 0.5635\n",
      "Epoch 2/10\n",
      "918295/918295 [==============================] - 81s 89us/sample - loss: 1.2266 - accuracy: 0.7556\n",
      "Epoch 3/10\n",
      " 91136/918295 [=>............................] - ETA: 1:14 - loss: 1.0641 - accuracy: 0.7847"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    x=(train_word_sequences, train_pos_sequences),\n",
    "    y=train_target,\n",
    "    batch_size=1024,\n",
    "    epochs=10,\n",
    "    validation_data=(\n",
    "        (dev_word_sequences, dev_pos_sequences),\n",
    "        dev_target\n",
    "    ),\n",
    "    validation_freq=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
